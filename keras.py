# -*- coding: utf-8 -*-
"""Keras.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xrcHQ8_nhkwYS2KZD08UaAciLLPOuw3s
"""

import tensorflow as tf
from tensorflow import keras

# Load the MNIST dataset
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

# Preprocess the data
x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)
x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)
x_train, x_test = x_train / 255.0, x_test / 255.0

# Define the model architecture
model = keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28, 1)),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dense(10, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam', 
              loss='sparse_categorical_crossentropy', 
              metrics=['accuracy'])

# Train the model
model.fit(x_train, y_train, epochs=5)

# Evaluate the model on the test data
test_loss, test_acc = model.evaluate(x_test, y_test)
print('Test Accuracy: ', test_acc)

"""This code defines and trains a simple neural network for classifying images of handwritten digits from the MNIST dataset. The network consists of a single fully connected hidden layer with 128 units and ReLU activation, followed by an output layer with 10 units and softmax activation, one for each class (0-9). The model is then compiled with the Adam optimizer, sparse categorical crossentropy loss function, and accuracy as the evaluation metric. Finally, the model is trained for 5 epochs on the training data and evaluated on the test data."""

from keras.models import Sequential
from keras.layers import Dense

# Initialize the model
model = Sequential()

# Add a hidden layer with 128 units and ReLU activation
model.add(Dense(128, activation='relu', input_shape=(30,)))

# Add a second hidden layer with 64 units and ReLU activation
model.add(Dense(64, activation='relu'))

# Add an output layer with a single unit and sigmoid activation
model.add(Dense(1, activation='sigmoid'))

# Compile the model with binary crossentropy loss and the Adam optimizer
model.compile(loss='binary_crossentropy', optimizer='adam')

"""In this example, we build a model with three fully connected (Dense) layers. The first layer has 128 units and takes an input shape of (30,), meaning it expects 30-dimensional vectors as input. The second and third layers have 64 and 1 units, respectively. The activation functions used are ReLU for the hidden layers and sigmoid for the output layer, as we're solving a binary classification problem. The model is then compiled with binary crossentropy loss and the Adam optimizer."""

from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

# Initialize the model
model = Sequential()

# Add a 2D convolution layer with 32 filters, a kernel size of (3,3), and ReLU activation
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))

# Add a max pooling layer with a pool size of (2,2)
model.add(MaxPooling2D(pool_size=(2, 2)))

# Flatten the output from the previous layer to a 1D vector
model.add(Flatten())

# Add a dense layer with 128 units and ReLU activation
model.add(Dense(128, activation='relu'))

# Add an output layer with 10 units and softmax activation
model.add(Dense(10, activation='softmax'))

# Compile the model with categorical crossentropy loss and the Adam optimizer
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

"""In this example, we build a model for a multi-class classification problem. The model starts with a 2D convolution layer with 32 filters, a kernel size of (3,3), and ReLU activation. This is followed by a max pooling layer with a pool size of (2,2). The output from the previous layer is then flattened to a 1D vector and passed through two fully connected (Dense) layers, with 128 and 10 units, respectively. The activation functions used are ReLU for the hidden layer and softmax for the output layer. The model is then compiled with categorical crossentropy loss and the Adam optimizer, and we report accuracy as a metric."""

from keras.models import Sequential
from keras.layers import LSTM, Dense

# Initialize the model
model = Sequential()

# Add a LSTM layer with 128 units
model.add(LSTM(128, input_shape=(timesteps, input_dim)))

# Add a dense layer with a single unit and sigmoid activation
model.add(Dense(1, activation='sigmoid'))

# Compile the model with binary crossentropy loss and the Adam optimizer
model.compile(loss='binary_crossentropy', optimizer='adam')

"""In this example, we build a binary classification RNN model using a single LSTM layer with 128 units. The input shape is specified as (timesteps, input_dim), where timesteps is the number of time steps in the input sequence and input_dim is the number of features per time step. The model has a single dense output layer with a single unit and a sigmoid activation function, as we're solving a binary classification problem. The model is then compiled with binary crossentropy loss and the Adam optimizer."""