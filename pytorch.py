# -*- coding: utf-8 -*-
"""PyTorch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NIxWbLo2ymHke4tzVdmzTxSnRvnKDSSj
"""

import torch

# Creating a tensor
x = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32)
print("Tensor x:", x)

# Tensor shape
print("Shape of x:", x.shape)

# Tensor operations
y = torch.tensor([2, 2, 2, 2, 2], dtype=torch.float32)
z = x + y
print("Tensor z (x + y):", z)

# Matrix multiplication
matrix1 = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32)
matrix2 = torch.tensor([[7, 8], [9, 10], [11, 12]], dtype=torch.float32)
result = torch.matmul(matrix1, matrix2)
print("Result of matrix multiplication: \n", result)

# Moving tensors to GPU
# x_cuda = x.to("cuda")
# y_cuda = y.to("cuda")
# z_cuda = x_cuda + y_cuda
# print("Tensor z_cuda (x_cuda + y_cuda) on GPU:", z_cuda)

# Reduction operations
sum = z.sum()
print("Sum of tensor z:", sum)
mean = z.mean()
print("Mean of tensor z:", mean)

"""This code demonstrates how to create a tensor, get its shape, perform element-wise operations, matrix multiplication, move tensors to GPU for acceleration, and perform reduction operations to calculate the sum and mean of a tensor. I have commented out the cuda section for simple model demonstration in Google Colab, however the basic structure is the same.

The major advantage of element-wise operations on tensors using PyTorch is efficiency and ease of use. PyTorch provides efficient implementations of element-wise operations, which can be performed quickly on large tensors. Additionally, PyTorch's intuitive and user-friendly API allows for easy implementation of complex mathematical operations on tensors. These operations can be performed on CPU or GPU for even greater speed and performance, making PyTorch an excellent choice for high-performance numerical computing and machine learning tasks.
"""

import torch

# Defining the activation functions
relu = torch.nn.ReLU()
sigmoid = torch.nn.Sigmoid()

# Creating the inputs and outputs
inputs = torch.tensor([[-1.0, 0.0, 1.0]])
outputs_relu = relu(inputs)
outputs_sigmoid = sigmoid(inputs)

# Printing the results
print("Relu Activation Results:", outputs_relu)
print("Sigmoid Activation Results:", outputs_sigmoid)

"""The PyTorch library in Python supports a range of activation functions, which are used to transform input values into outputs. Activation functions are used to introduce non-linearity in the neural network, making it possible to model more complex relationships between inputs and outputs.

The primary use cases for activation functions include:

Improving the accuracy of the neural network: Activation functions help to train the model better by introducing non-linearity into the neural network, and thereby increasing the accuracy of the predictions.

Introducing sparsity: Activation functions allow the network to achieve sparse activation patterns, which can help to reduce the number of parameters in the network and improve the performance.

Improving the generalization capability of the model: Activation functions introduce non-linearity in the neural network, which helps to improve the generalization capability of the network.

Offering interpretability: By introducing non-linearity, activation functions make it easier to interpret the results of the neural network.
"""

import torch
import torch.nn as nn

# Define a model
model = nn.Sequential(nn.Linear(10, 10), nn.ReLU(), nn.Linear(10, 10))

# Generate some random data for input and target
inputs = torch.randn(10, 10)
targets = torch.randint(0, 10, (10,))

# Pass the input through the model to get the outputs
outputs = model(inputs)

# Define the loss functions
cross_entropy_loss = nn.CrossEntropyLoss()
mean_squared_error_loss = nn.MSELoss()
smooth_l1_loss = nn.SmoothL1Loss()

# Calculate the losses
ce_loss = cross_entropy_loss(outputs, targets)
mse_loss = mean_squared_error_loss(outputs, targets.float())
sl1_loss = smooth_l1_loss(outputs, targets.float())

print("Cross Entropy Loss:", ce_loss.item())
print("Mean Squared Error Loss:", mse_loss.item())
print("Smooth L1 Loss:", sl1_loss.item())

"""In this example, we define a simple neural network using the nn.Sequential class, and generate some random data for input and target. We pass the input through the model to get the outputs, and then define three loss functions: nn.CrossEntropyLoss, nn.MSELoss, and nn.SmoothL1Loss. Finally, we calculate the losses by calling the loss functions on the outputs and targets, and print out the result."""

import torch
import torch.nn as nn
import torch.optim as optim

# Define a model
model = nn.Sequential(nn.Linear(10, 10), nn.ReLU(), nn.Linear(10, 10))

# Define the loss function
criterion = nn.CrossEntropyLoss()

# Generate some random data for input and target
inputs = torch.randn(32, 10)
targets = torch.randint(0, 10, (32,))

# Define the optimizers
sgd_optimizer = optim.SGD(model.parameters(), lr=0.01)
adam_optimizer = optim.Adam(model.parameters(), lr=0.01)
rmsprop_optimizer = optim.RMSprop(model.parameters(), lr=0.01)

# Loop for a number of epochs
for epoch in range(10):
    # Pass the input through the model to get the outputs
    outputs = model(inputs)
    
    # Calculate the loss
    loss = criterion(outputs, targets)
    
    # Zero the gradients
    sgd_optimizer.zero_grad()
    adam_optimizer.zero_grad()
    rmsprop_optimizer.zero_grad()
    
    # Calculate the gradients
    loss.backward()
    
    # Update the parameters using different optimizers
    sgd_optimizer.step()
    adam_optimizer.step()
    rmsprop_optimizer.step()
    
    print("Epoch:", epoch+1, "SGD Loss:", loss.item(), "Adam Loss:", loss.item(), "RMSprop Loss:", loss.item())

"""In this example, we define a simple neural network using the nn.Sequential class, and a cross-entropy loss using nn.CrossEntropyLoss. We generate some random data for input and target, and define three optimizers: optim.SGD, optim.Adam, and optim.RMSprop. In a loop that runs for 10 epochs, we pass the input through the model to get the outputs, calculate the loss, and update the parameters using the different optimizers. The gradients are calculated using loss.backward() and the optimizers' step() method is called to update the parameters."""

import torch
import torch.nn as nn
import torch.optim as optim

# Define a model
model = nn.Sequential(nn.Linear(10, 10), nn.ReLU(), nn.Linear(10, 10))

# Define the loss function
criterion = nn.CrossEntropyLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.01)

# Generate some random data for input and target
inputs = torch.randn(32, 10)
targets = torch.randint(0, 10, (32,))

# Loop for a number of epochs
for epoch in range(10):
    # Pass the input through the model to get the outputs
    outputs = model(inputs)
    
    # Calculate the loss
    loss = criterion(outputs, targets)
    
    # Zero the gradients
    optimizer.zero_grad()
    
    # Calculate the gradients
    loss.backward()
    
    # Update the parameters
    optimizer.step()
    
    print("Epoch:", epoch+1, "Loss:", loss.item())

